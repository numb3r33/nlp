# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_task_text_sim.ipynb (unless otherwise specified).

__all__ = ['Pad_Chunk_Pair', 'Undict', 'BertTextCNN', 'TextSimTokenizer', 'BertTextCNNCallback', 'BertTextCNNLearner']

# Cell
from ..data.external import load_medical_questions_pair
from fastai.text.all   import *
from inspect           import signature

# Cell
class Pad_Chunk_Pair(ItemTransform):
    "Pad `samples` by adding padding by chunks of size `seq_len`"
    def __init__(self, pad_idx=1, pad_first=True, seq_len=72,decode=True,**kwargs):
        store_attr('pad_idx, pad_first, seq_len')
        super().__init__(**kwargs)
    def before_call(self, b):
        "Set `self.max_len` before encodes"
        xas = []
        for xs in b:
            xa, xb, xc = xs[0]
            xas.append(xa.shape[0])

        self.max_len = max(xas)

    def __call__(self, b, **kwargs):
        self.before_call(b)
        return super().__call__(tuple(b), **kwargs)

    def encodes(self, batch):
        texts  = ([s[0][0] for s in batch], [s[0][1] for s in batch], [s[0][2] for s in batch])
        labels = default_collate([s[1:] for s in batch])

        inps   = {}

        pa = default_collate([pad_chunk(ta, pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len) for ta in texts[0]])
        pb = default_collate([pad_chunk(tb, pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len) for tb in texts[1]])
        pc = default_collate([pad_chunk(tc, pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len) for tc in texts[2]])

        inps['pa'] = pa
        inps['pb'] = pb
        inps['pc'] = pc


        if len(labels):
            inps['labels'] = labels[0]

        res = (inps, )

        return res

# Cell
class Undict(Transform):
    def decodes(self, x:dict):
        if 'pa' in x and 'pb' in x and 'pc' in x: res = (x['pa'], x['pb'], x['pc'], x['labels'])
        return res

# Cell
from transformers import BertModel

# Cell
class BertTextCNN(Module):
  """
  https://github.com/yechens/COVID-19-sentence-pair/blob/main/code/model.py
  """
  def __init__(self,
               dim=768,
               add_edit_dist=False,
               pool_way='avg',
               weight=[1.,1.],
               filter_num=128,
               filter_sizes=[2,3,4],
               smoothing=0.05):

    self.bert    = BertModel.from_pretrained('bert-base-cased')
    self.dropout = nn.Dropout(0.1)

    # textcnn
    class_num  = 2
    chanel_num = 1

    self.convs = nn.ModuleList(
        [nn.Conv2d(chanel_num, filter_num, (size, dim)) for size in filter_sizes])
    self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num)

    self.reset_parameters()
    self.pool_way = pool_way

  def reset_parameters(self):
      nn.init.xavier_uniform_(self.fc.weight)
      nn.init.constant_(self.fc.bias, 0.0)

  def forward(self, input_ids, token_type_ids, attention_mask):
    out = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
    bert_enc    = self.dropout(out.last_hidden_state)

    x = bert_enc.unsqueeze(1)
    x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]

    if self.pool_way == 'max':
        x = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in x]
    elif self.pool_way == 'avg':
        x = [F.avg_pool1d(item, item.size(2)).squeeze(2) for item in x]

    x      = torch.cat(x, 1)
    x      = self.dropout(x)
    logits = self.fc(x)
    return logits

# Cell
class TextSimTokenizer(Transform):
  def __init__(self, tokenizer):
    self.tokenizer = tokenizer
  def encodes(self, x):
    encoded_dict = self.tokenizer(x['question_1'], x['question_2'],
                                  padding=True
                                  )

    return (tensor(encoded_dict['input_ids']), tensor(encoded_dict['token_type_ids']), tensor(encoded_dict['attention_mask']))

  def decodes(self,x): return TitledStr(self.tokenizer.decode(x[0].cpu().numpy()))

# Cell
class BertTextCNNCallback(Callback):
    def __init__(self, model):
        self.labels = tuple()
        self.model_args = {k:v.default for k, v in signature(model.forward).parameters.items()}

    def before_batch(self):
        if 'labels' in self.xb[0].keys():
            self.labels = (self.xb[0]['labels'], )
        # make a tuple containing an element for each argument model excepts
        # if argument is not in xb it is set to default value
        self.learn.xb = tuple([self.xb[0]['pa'], self.xb[0]['pb'], self.xb[0]['pc']])

    def after_pred(self):
        if len(self.labels):
            loss = self.learn.loss_func(self.pred, self.labels[0]).clone()

            self.learn.loss_grad = loss
            self.learn.loss      = loss.clone()

        self.learn.pred = self.pred

    def after_loss(self):
        if len(self.labels):

            self.learn.yb = (self.labels[0], )
            self.labels   = tuple()

# Cell
@delegates(Learner.__init__)
class BertTextCNNLearner(Learner):
    "Learner for training transformers from HuggingFace"
    def __init__(self, dls, model, **kwargs):
        super().__init__(dls, model, **kwargs)
        self.add_cb(BertTextCNNCallback(model))